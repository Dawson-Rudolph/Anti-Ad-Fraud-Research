{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection using XGBoost\n",
    "\n",
    "This notebook implements an XGBoost model for detecting fraudulent ad clicks using a preprocessed dataset. It includes loading data, handling class imbalance, training the model, and saving it for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "This section imports required libraries:\n",
    "- `numpy`, `pandas`: For data manipulation.\n",
    "- `xgboost`: To train the classification model.\n",
    "- `matplotlib`: For visualizing feature importance.\n",
    "- `train_test_split`: For splitting the dataset into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "# Assuming the training_preprocess.py file is in the same directory\n",
    "# from training_preprocess import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Preprocessed Data\n",
    "The preprocessed dataset is loaded using a custom function (`train_to_df`), and the first few rows are displayed to confirm proper loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the preprocessed data\n",
    "from data_reprocessing import train_to_df\n",
    "train_file = 'new_train.csv'  \n",
    "dataset = train_to_df(train_file)\n",
    "# Display the first few rows of the dataset to verify the preprocessing\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Data Distribution\n",
    "Fraudulent and non-fraudulent clicks are counted, and their percentages are calculated to understand the class imbalance in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of fraudulent and non-fraudulent clicks\n",
    "fraud_counts = dataset['is_attributed'].value_counts()\n",
    "fraud_percentage = fraud_counts[0] / len(dataset) * 100\n",
    "non_fraud_percentage = fraud_counts[1] / len(dataset) * 100\n",
    "\n",
    "print(f\"Fraudulent clicks: {fraud_counts[0]} ({fraud_percentage:.2f}%)\")\n",
    "print(f\"Non-fraudulent clicks: {fraud_counts[1]} ({non_fraud_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting Data into Features and Target\n",
    "- `X`: Contains all features except the target column (`is_attributed`).\n",
    "- `y`: The target variable representing fraudulent (1) or non-fraudulent (0) clicks.\n",
    "The data is split into training (80%) and validation (20%) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "X = dataset.drop(columns=['is_attributed'])  # Features\n",
    "y = dataset['is_attributed']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of training and validation sets\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Validation data shape: \", X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining XGBoost Model Parameters\n",
    "The XGBoost parameters are configured for efficient training, class imbalance handling, and generalization. Key parameters:\n",
    "- `eta`: Controls learning rate.\n",
    "- `scale_pos_weight`: Balances the weight for the minority class.\n",
    "- `max_depth`: Limits the depth of trees to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'eta': 0.3,  # Learning rate; faster training, potential overfitting risk\n",
    "    'tree_method': 'hist',  # Faster tree-building for large datasets\n",
    "    'max_depth': 6,  # Limits tree depth to prevent overfitting\n",
    "    'subsample': 0.9,  # Uses 90% of data per tree to improve generalization\n",
    "    'colsample_bytree': 0.7,  # Uses 70% of features to reduce overfitting\n",
    "    'objective': 'binary:logistic',  # Binary classification (fraud detection)\n",
    "    'eval_metric': 'auc',  # AUC metric for imbalanced data performance\n",
    "    'scale_pos_weight': 9,  # Increases weight for minority class (real clicks)\n",
    "    'nthread': 8,  # Parallel training for faster computation\n",
    "    'random_state': 42  # Ensures reproducibility of results\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the XGBoost Model\n",
    "The model is trained with early stopping to prevent overfitting. Training and validation datasets are converted to DMatrix format for optimized computation. The best iteration is printed.\n",
    "\n",
    "Visualizing Feature Importance\n",
    "Feature importance is plotted to identify the most significant predictors contributing to the classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and validation datasets to DMatrix format (optimized for XGBoost)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "# Train the model with early stopping\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "model = xgb.train(params, dtrain, num_boost_round=100, evals=watchlist, early_stopping_rounds=10, verbose_eval=10)\n",
    "\n",
    "# Display the best iteration\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# Plot feature importance to see the most significant features\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making Predictions and Saving the Model\n",
    "- Predictions are generated on the validation set.\n",
    "- The trained model is saved as `xgboost_model.json`.\n",
    "- The first few predictions are displayed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "if hasattr(model, 'best_ntree_limit'):\n",
    "    # Use best_ntree_limit if early stopping is applied\n",
    "    y_pred = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "else:\n",
    "    # Use all trees if early stopping is not applied\n",
    "    y_pred = model.predict(dvalid)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save_model('xgboost_model.json')\n",
    "\n",
    "# Display the first few predictions\n",
    "print(y_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates a streamlined approach to implementing XGBoost for fraud detection. The saved model can be used for deployment or further testing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
